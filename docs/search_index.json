[["data-analysis-and-visualization-using-r.html", "Conducting Empirical Research in Economics Chapter 1 Data Analysis and Visualization using R 1.1 Introduction to R and R Studio 1.2 Data transformation 1.3 Exploratory data analysis 1.4 Descriptive analysis 1.5 Data Visualization", " Conducting Empirical Research in Economics Mohammed Seid 2022-06-05 Chapter 1 Data Analysis and Visualization using R knitr::include_graphics(here::here(&quot;image&quot;, &quot;trans.png&quot;)) Figure 1.1: Data anaylsis piepleine 1.1 Introduction to R and R Studio knitr::include_graphics(here::here(&quot;image&quot;, &quot;appstore.png&quot;)) Figure 1.2: R vs R Studio 1.2 Data transformation 1.2.1 The tidyverse The tidyverse is a collection of R packages that share common philosophies and are designed to work together. knitr::include_graphics(here::here(&quot;image&quot;, &quot;tidyverse.png&quot;)) Figure 1.3: tidyverse packages 1.2.1.1 readr and haven 1.2.1.2 The pipe %&gt;% A way to chain together commands Can be read as “and then” when reading over code knitr::include_graphics(here::here(&quot;image&quot;, &quot;pipe.jpg&quot;)) Figure 1.4: The Pipe Operator 1.2.1.3 tidyr knitr::include_graphics(here::here(&quot;image&quot;, &quot;tidy.png&quot;)) Figure 1.5: Tidy Data 1.2.1.4 dplyr The Six Main Verbs (6MV) of data wrangling filter() select() summarize() group_by() mutate() arrange() 1.2.1.4.1 filter() Select a subset of the rows of a data frame. Arguments are “filters” that you’d like to apply. Use == to compare a variable to a value. Use | to check for any in multiple filters being true. Use , to check for all of multiple filters being true. Use %in% to check for any being true (shortcut to using | repeatedly with ==) 1.2.1.4.2 select() select variables (columns within a dataframe). : a range of consecutive variables. -starts_with(): variables Starts with a prefix. -ends_with(): variables ends with a suffix. -contains(): Contains a literal string. 1.2.1.4.3 summarize() Any numerical summary that you want to apply to a column of a data frame is specified within summarize(). 1.2.1.4.4 Combining summarize() with group_by() When you’d like to determine a numerical summary for all levels of a different categorical variable 1.2.1.4.5 mutate() Allows you to create a new variable with a specific value create a new variable based on other variables change the contents of an existing variable 1.2.1.4.6 arrange() Reorders the rows in a data frame based on the values of one or more variables Can also put into descending order 1.3 Exploratory data analysis 1.3.1 gtsummary Package 1.4 Descriptive analysis 1.5 Data Visualization "],["regression-analysis-using-r.html", "Chapter 2 Regression analysis using R 2.1 Analysis of cross-sectional data 2.2 Analysis of categorical Data 2.3 Analysis of timeseries data 2.4 Analysis of panel data", " Chapter 2 Regression analysis using R 2.1 Analysis of cross-sectional data 2.2 Analysis of categorical Data 2.3 Analysis of timeseries data 2.4 Analysis of panel data "],["causal-analysis-using-r.html", "Chapter 3 Causal analysis using R 3.1 Introduction to causality 3.2 Randomized experiment 3.3 Instrumental variables 3.4 Matching and subclassification 3.5 Panel data and fixed effect 3.6 Difference-in-Difference 3.7 Regression discontinuity Design 3.8 Heterogenous Treatment Effect 3.9 Propensity score matching", " Chapter 3 Causal analysis using R 3.1 Introduction to causality 3.2 Randomized experiment 3.3 Instrumental variables 3.4 Matching and subclassification 3.5 Panel data and fixed effect 3.6 Difference-in-Difference 3.7 Regression discontinuity Design 3.8 Heterogenous Treatment Effect 3.8.1 Endogenous Switching Regression 3.8.1.1 Financail inclusion &amp; HH welfare: Application using Ethiopia Socio Economic Survey 3.9 Propensity score matching 3.9.1 load the dataset "],["geospatial-analytics-using-r.html", "Chapter 4 Geospatial analytics using R 4.1 Core concepts, vector data, and plotting 4.2 Spatial analysis 4.3 Raster data", " Chapter 4 Geospatial analytics using R 4.1 Core concepts, vector data, and plotting 4.2 Spatial analysis 4.3 Raster data "],["advanced-r.html", "Chapter 5 Advanced R 5.1 Machine learning 5.2 Deep learning 5.3 Text mining", " Chapter 5 Advanced R 5.1 Machine learning 5.1.1 Random Forest library(caret) ## Loading required package: ggplot2 ## Loading required package: lattice library(rpart.plot) ## Loading required package: rpart library(tidymodels) ## ── Attaching packages ────────────────────────────────────── tidymodels 0.2.0 ── ## ✔ broom 0.8.0 ✔ rsample 0.1.1 ## ✔ dials 0.1.1 ✔ tibble 3.1.7 ## ✔ dplyr 1.0.9 ✔ tidyr 1.2.0 ## ✔ infer 1.0.0 ✔ tune 0.2.0 ## ✔ modeldata 0.1.1 ✔ workflows 0.2.6 ## ✔ parsnip 0.2.1 ✔ workflowsets 0.2.1 ## ✔ purrr 0.3.4 ✔ yardstick 0.0.9 ## ✔ recipes 0.2.0 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ purrr::lift() masks caret::lift() ## ✖ yardstick::precision() masks caret::precision() ## ✖ dials::prune() masks rpart::prune() ## ✖ yardstick::recall() masks caret::recall() ## ✖ yardstick::sensitivity() masks caret::sensitivity() ## ✖ yardstick::specificity() masks caret::specificity() ## ✖ recipes::step() masks stats::step() ## • Search for functions across packages at https://www.tidymodels.org/find/ library(vip) ## ## Attaching package: &#39;vip&#39; ## The following object is masked from &#39;package:utils&#39;: ## ## vi library(haven) library(here) ## here() starts at D:/Github/economics_research library(ranger) df &lt;- read_dta(here(&quot;data&quot;, &quot;master_data2.dta&quot;)) df &lt;- df %&gt;% dplyr::select(household_id, lntot, lneduc, lntot, finance, mobile_own, educ, iddir, marital, religion, age, ag2, gender, hh_size, hh_size2, know_account, wage_self, credit_info, urban, bank_com, educ_com, lndis) %&gt;% drop_na() data&lt;-as_factor(df) summary(data) ## household_id lntot lneduc ## Length:6598 Min. : 5.583 Min. : 0.000 ## Class :character 1st Qu.: 9.154 1st Qu.: 0.000 ## Mode :character Median : 9.689 Median : 3.798 ## Mean : 9.681 Mean : 3.176 ## 3rd Qu.:10.205 3rd Qu.: 5.668 ## Max. :13.038 Max. :12.449 ## ## finance mobile_own educ iddir ## No Access to Finance:3573 No :3037 Non education:3016 No :3632 ## Access to Finance :3025 Yes:3561 Primary :2093 Yes:2966 ## secondary : 492 ## certificate : 131 ## diploma : 373 ## degree : 382 ## postgraduate : 111 ## marital religion age ag2 ## Non married:2016 Non Muslim:4123 Min. :18.00 Min. : 324 ## Married :4582 Muslim :2475 1st Qu.:30.00 1st Qu.: 900 ## Median :40.00 Median :1600 ## Mean :42.26 Mean :2013 ## 3rd Qu.:52.00 3rd Qu.:2704 ## Max. :99.00 Max. :9801 ## ## gender hh_size hh_size2 know_account wage_self ## Female:2096 Min. : 1.000 Min. : 1.00 No :3132 No :4176 ## Male :4502 1st Qu.: 3.000 1st Qu.: 9.00 Yes:3466 Yes:2422 ## Median : 4.000 Median : 16.00 ## Mean : 4.242 Mean : 23.24 ## 3rd Qu.: 6.000 3rd Qu.: 36.00 ## Max. :19.000 Max. :361.00 ## ## credit_info urban bank_com educ_com lndis ## No :5501 Rural:3002 NO :3846 Min. :0.0000 Min. :0.0000 ## Yes:1097 Urban:3596 Yes:2752 1st Qu.:0.3333 1st Qu.:0.6931 ## Median :0.9333 Median :1.6094 ## Mean :1.1425 Mean :1.9640 ## 3rd Qu.:1.7333 3rd Qu.:3.0445 ## Max. :4.6000 Max. :6.3986 ## split the data into trainng (75%) and testing (25%) set.seed(6598) data_split &lt;- initial_split(data, prop = 3/4) extract training and testing sets data_train &lt;- training(data_split) data_test &lt;- testing(data_split) At some point we’re going to want to do some parameter tuning, and to do that we’re going to want to use cross-validation. So we can create a cross-validated version of the training set in preparation for that moment using vfold_cv(). create CV object from training data data_cv &lt;- vfold_cv(data_train) define the recipe data_recipe &lt;- # which consists of the formula (outcome ~ predictors) recipe(finance ~ mobile_own + educ + iddir + marital + religion + age + ag2 + gender + hh_size + hh_size2 + know_account + wage_self + credit_info + urban + bank_com + educ_com + lndis, data = data) %&gt;% # and some pre-processing steps step_normalize(all_numeric()) %&gt;% step_impute_knn(all_predictors()) rf_model &lt;- # specify that the model is a random forest rand_forest() %&gt;% # specify that the `mtry` parameter needs to be tuned set_args(mtry = tune()) %&gt;% # select the engine/package that underlies the model set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) %&gt;% # choose either the continuous regression or binary classification mode set_mode(&quot;classification&quot;) set the workflow rf_workflow &lt;- workflow() %&gt;% # add the recipe add_recipe(data_recipe) %&gt;% # add the model add_model(rf_model) specify which values eant to try rf_grid &lt;- expand.grid(mtry = c(3, 4, 5)) extract results rf_tune_results &lt;- rf_workflow %&gt;% tune_grid(resamples = data_cv, #CV object grid = rf_grid, # grid of values to try metrics = metric_set(accuracy, roc_auc) # metrics we care about ) print results rf_tune_results %&gt;% collect_metrics() ## # A tibble: 6 × 7 ## mtry .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3 accuracy binary 0.842 10 0.00361 Preprocessor1_Model1 ## 2 3 roc_auc binary 0.910 10 0.00326 Preprocessor1_Model1 ## 3 4 accuracy binary 0.841 10 0.00402 Preprocessor1_Model2 ## 4 4 roc_auc binary 0.910 10 0.00345 Preprocessor1_Model2 ## 5 5 accuracy binary 0.843 10 0.00394 Preprocessor1_Model3 ## 6 5 roc_auc binary 0.910 10 0.00349 Preprocessor1_Model3 param_final &lt;- rf_tune_results %&gt;% select_best(metric = &quot;accuracy&quot;) param_final ## # A tibble: 1 × 2 ## mtry .config ## &lt;dbl&gt; &lt;chr&gt; ## 1 5 Preprocessor1_Model3 rf_workflow &lt;- rf_workflow %&gt;% finalize_workflow(param_final) rf_fit &lt;- rf_workflow %&gt;% # fit on the training set and evaluate on test set last_fit(data_split) test_performance &lt;- rf_fit %&gt;% collect_metrics() test_performance ## # A tibble: 2 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.856 Preprocessor1_Model1 ## 2 roc_auc binary 0.923 Preprocessor1_Model1 Overall the performance is very good, with an accuracy of 0.83 and an AUC of 0.903. generate predictions from the test set test_predictions &lt;- rf_fit %&gt;% collect_predictions() generate a confusion matrix test_predictions %&gt;% conf_mat(truth = finance, estimate = .pred_class) ## Truth ## Prediction No Access to Finance Access to Finance ## No Access to Finance 749 112 ## Access to Finance 125 664 We could also plot distributions of the predicted probability distributions for each class. colnames(test_predictions) ## [1] &quot;id&quot; &quot;.pred_No Access to Finance&quot; ## [3] &quot;.pred_Access to Finance&quot; &quot;.row&quot; ## [5] &quot;.pred_class&quot; &quot;finance&quot; ## [7] &quot;.config&quot; test_predictions %&gt;% ggplot() + geom_density(aes(x = `.pred_Access to Finance`, fill = finance), alpha = 0.5) Fitting and using your final model # the last model last_rf_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) %&gt;% set_mode(&quot;classification&quot;) rf_model &lt;- # specify that the model is a random forest rand_forest() %&gt;% # specify that the `mtry` parameter needs to be tuned set_args(mtry = tune()) %&gt;% # select the engine/package that underlies the model set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) %&gt;% # choose either the continuous regression or binary classification mode set_mode(&quot;classification&quot;) the last workflow last_rf_workflow &lt;- rf_workflow %&gt;% update_model(last_rf_mod) the last fit set.seed(6598) last_rf_fit &lt;- last_rf_workflow %&gt;% last_fit(data_split) last_rf_fit %&gt;% collect_metrics() ## # A tibble: 2 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.856 Preprocessor1_Model1 ## 2 roc_auc binary 0.923 Preprocessor1_Model1 last_rf_fit %&gt;% pluck(&quot;.workflow&quot;, 1) %&gt;% extract_fit_parsnip() %&gt;% vip(num_features = 20) 5.1.2 Logistic Regression df &lt;- read_dta(here(&quot;data&quot;, &quot;master_data2.dta&quot;)) df &lt;- df %&gt;% select(finance, mobile_own, educ, iddir, marital, religion, age, ag2, gender, hh_size, hh_size2, know_account, wage_self, credit_info, urban, bank_com, educ_com, lndis) %&gt;% drop_na() data&lt;-as_factor(df) set.seed(345) data_split &lt;- initial_split(data, prop = 0.75, strata = finance) data_training &lt;- data_split %&gt;% training() data_test &lt;- data_split %&gt;% testing() data_recipe &lt;- recipe(finance ~ ., data = data_training) %&gt;% step_YeoJohnson(all_numeric(), -all_outcomes()) %&gt;% step_normalize(all_numeric(), -all_outcomes()) %&gt;% step_dummy(all_nominal(), -all_outcomes()) data_recipe %&gt;% prep() %&gt;% bake(new_data = data_training) ## # A tibble: 4,947 × 23 ## age ag2 hh_size hh_size2 educ_com lndis finance mobile_own_Yes ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0.0338 0.0341 0.448 0.457 -1.03 0.795 Access to Fin… 0 ## 2 0.834 0.834 2.30 2.26 -1.03 0.781 Access to Fin… 0 ## 3 0.988 0.988 0.448 0.457 -1.03 0.781 Access to Fin… 0 ## 4 -0.526 -0.525 0.448 0.457 -1.03 0.809 Access to Fin… 0 ## 5 -0.526 -0.525 -0.459 -0.466 -0.395 0.500 Access to Fin… 0 ## 6 -1.12 -1.12 -1.04 -1.06 -0.395 0.600 Access to Fin… 1 ## 7 0.613 0.613 1.77 1.76 -0.395 0.500 Access to Fin… 0 ## 8 0.670 0.670 1.48 1.48 -0.395 0.500 Access to Fin… 1 ## 9 0.307 0.307 1.48 1.48 -0.975 0.208 Access to Fin… 1 ## 10 -1.01 -1.01 -1.04 -1.06 -0.0734 0.809 Access to Fin… 1 ## # … with 4,937 more rows, and 15 more variables: educ_Primary &lt;dbl&gt;, ## # educ_secondary &lt;dbl&gt;, educ_certificate &lt;dbl&gt;, educ_diploma &lt;dbl&gt;, ## # educ_degree &lt;dbl&gt;, educ_postgraduate &lt;dbl&gt;, iddir_Yes &lt;dbl&gt;, ## # marital_Married &lt;dbl&gt;, religion_Muslim &lt;dbl&gt;, gender_Male &lt;dbl&gt;, ## # know_account_Yes &lt;dbl&gt;, wage_self_Yes &lt;dbl&gt;, credit_info_Yes &lt;dbl&gt;, ## # urban_Urban &lt;dbl&gt;, bank_com_Yes &lt;dbl&gt; model specification logistic_model &lt;- logistic_reg() %&gt;% set_engine(&#39;glm&#39;) %&gt;% set_mode(&#39;classification&#39;) create workflow data_wf &lt;- workflow() %&gt;% add_model(logistic_model) %&gt;% add_recipe(data_recipe) fit the model data_logistic_fit &lt;- data_wf %&gt;% fit(data = data_training) exploring trained data data_trained_model &lt;- data_logistic_fit %&gt;% extract_fit_parsnip() Variable importance vip(data_trained_model) evaluate perfromance predictions_categories &lt;- predict(data_logistic_fit, new_data = data_test) predictions_categories ## # A tibble: 1,651 × 1 ## .pred_class ## &lt;fct&gt; ## 1 No Access to Finance ## 2 Access to Finance ## 3 No Access to Finance ## 4 No Access to Finance ## 5 No Access to Finance ## 6 No Access to Finance ## 7 Access to Finance ## 8 Access to Finance ## 9 No Access to Finance ## 10 Access to Finance ## # … with 1,641 more rows predictions_probabilities &lt;- predict(data_logistic_fit, new_data = data_test, type = &#39;prob&#39;) predictions_probabilities ## # A tibble: 1,651 × 2 ## `.pred_No Access to Finance` `.pred_Access to Finance` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.942 0.0577 ## 2 0.383 0.617 ## 3 0.856 0.144 ## 4 0.937 0.0628 ## 5 0.865 0.135 ## 6 0.920 0.0804 ## 7 0.311 0.689 ## 8 0.267 0.733 ## 9 0.857 0.143 ## 10 0.125 0.875 ## # … with 1,641 more rows combine together test_results &lt;- data_test %&gt;% select(finance) %&gt;% bind_cols(predictions_categories) %&gt;% bind_cols(predictions_probabilities) test_results ## # A tibble: 1,651 × 4 ## finance .pred_class `.pred_No Access…` `.pred_Access …` ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Access to Finance No Access to Finance 0.942 0.0577 ## 2 Access to Finance Access to Finance 0.383 0.617 ## 3 No Access to Finance No Access to Finance 0.856 0.144 ## 4 No Access to Finance No Access to Finance 0.937 0.0628 ## 5 No Access to Finance No Access to Finance 0.865 0.135 ## 6 No Access to Finance No Access to Finance 0.920 0.0804 ## 7 Access to Finance Access to Finance 0.311 0.689 ## 8 No Access to Finance Access to Finance 0.267 0.733 ## 9 No Access to Finance No Access to Finance 0.857 0.143 ## 10 Access to Finance Access to Finance 0.125 0.875 ## # … with 1,641 more rows confusion matrix conf_mat(test_results, truth = finance, estimate = .pred_class) ## Truth ## Prediction No Access to Finance Access to Finance ## No Access to Finance 740 98 ## Access to Finance 154 659 f score f_meas(test_results, truth = finance, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 f_meas binary 0.855 colnames(test_results) ## [1] &quot;finance&quot; &quot;.pred_class&quot; ## [3] &quot;.pred_No Access to Finance&quot; &quot;.pred_Access to Finance&quot; ROC curve roc_curve(test_results, truth = finance, estimate = `.pred_Access to Finance`) %&gt;% autoplot() roc_auc(test_results, truth = finance, `.pred_Access to Finance`) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.0923 5.1.3 Linear Regression library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ readr 2.1.2 ✔ forcats 0.5.1 ## ✔ stringr 1.4.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ stringr::fixed() masks recipes::fixed() ## ✖ dplyr::lag() masks stats::lag() ## ✖ purrr::lift() masks caret::lift() ## ✖ readr::spec() masks yardstick::spec() library(tidymodels) library(vip) # for variable importance library(haven) load data set df &lt;- read_dta(here(&quot;data&quot;, &quot;master_data2.dta&quot;)) df &lt;- df %&gt;% select(lntot, lneduc, lntot, finance, mobile_own, educ, iddir, marital, religion, age, ag2, gender, hh_size, hh_size2, know_account, wage_self, credit_info, urban, bank_com, educ_com, lndis) %&gt;% drop_na() data&lt;-as_factor(df) set.seed(314) # Create a split object data_split &lt;- initial_split(data, prop = 0.75, strata = lntot) # Build training data set data_training &lt;- data_split %&gt;% training() # Build testing data set data_test &lt;- data_split %&gt;% testing() Model Specification lm_model &lt;- linear_reg() %&gt;% set_engine(&#39;lm&#39;) %&gt;% # adds lm implementation of linear regression set_mode(&#39;regression&#39;) #View object properties lm_model ## Linear Regression Model Specification (regression) ## ## Computational engine: lm Fitting to Training Data lm_fit &lt;- lm_model %&gt;% fit(lntot ~ ., data = data_training) # View lm_fit properties lm_fit ## parsnip model object ## ## ## Call: ## stats::lm(formula = lntot ~ ., data = data) ## ## Coefficients: ## (Intercept) lneduc financeAccess to Finance ## 9.854e+00 1.573e-02 1.466e-01 ## mobile_ownYes educPrimary educsecondary ## 1.559e-01 7.871e-02 4.243e-02 ## educcertificate educdiploma educdegree ## 1.727e-01 1.238e-01 2.171e-01 ## educpostgraduate iddirYes maritalMarried ## 3.541e-01 1.647e-02 7.008e-02 ## religionMuslim age ag2 ## 2.192e-01 -4.738e-03 6.036e-05 ## genderMale hh_size hh_size2 ## -2.133e-02 -2.739e-01 1.283e-02 ## know_accountYes wage_selfYes credit_infoYes ## 7.837e-02 4.397e-02 1.403e-01 ## urbanUrban bank_comYes educ_com ## 1.026e-01 1.014e-01 1.433e-01 ## lndis ## 2.211e-02 Exploring Training Results names(lm_fit) ## [1] &quot;lvl&quot; &quot;spec&quot; &quot;fit&quot; &quot;preproc&quot; &quot;elapsed&quot; summary(lm_fit$fit) ## ## Call: ## stats::lm(formula = lntot ~ ., data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8906 -0.3765 -0.0121 0.3522 3.1539 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.854e+00 8.255e-02 119.369 &lt; 2e-16 *** ## lneduc 1.573e-02 3.259e-03 4.827 1.43e-06 *** ## financeAccess to Finance 1.466e-01 2.418e-02 6.064 1.43e-09 *** ## mobile_ownYes 1.559e-01 1.887e-02 8.264 &lt; 2e-16 *** ## educPrimary 7.871e-02 2.380e-02 3.306 0.000952 *** ## educsecondary 4.243e-02 4.058e-02 1.046 0.295806 ## educcertificate 1.727e-01 6.969e-02 2.478 0.013230 * ## educdiploma 1.238e-01 4.432e-02 2.793 0.005250 ** ## educdegree 2.171e-01 4.706e-02 4.613 4.07e-06 *** ## educpostgraduate 3.541e-01 7.439e-02 4.760 1.99e-06 *** ## iddirYes 1.647e-02 1.829e-02 0.901 0.367817 ## maritalMarried 7.008e-02 2.506e-02 2.797 0.005186 ** ## religionMuslim 2.192e-01 1.885e-02 11.629 &lt; 2e-16 *** ## age -4.738e-03 3.302e-03 -1.435 0.151387 ## ag2 6.036e-05 3.280e-05 1.840 0.065829 . ## genderMale -2.133e-02 2.306e-02 -0.925 0.354870 ## hh_size -2.739e-01 1.467e-02 -18.678 &lt; 2e-16 *** ## hh_size2 1.283e-02 1.260e-03 10.181 &lt; 2e-16 *** ## know_accountYes 7.837e-02 2.494e-02 3.142 0.001687 ** ## wage_selfYes 4.397e-02 1.960e-02 2.243 0.024943 * ## credit_infoYes 1.403e-01 2.539e-02 5.527 3.42e-08 *** ## urbanUrban 1.026e-01 2.842e-02 3.609 0.000310 *** ## bank_comYes 1.014e-01 2.634e-02 3.849 0.000120 *** ## educ_com 1.433e-01 1.465e-02 9.780 &lt; 2e-16 *** ## lndis 2.211e-02 8.476e-03 2.609 0.009110 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5892 on 4921 degrees of freedom ## Multiple R-squared: 0.448, Adjusted R-squared: 0.4453 ## F-statistic: 166.4 on 24 and 4921 DF, p-value: &lt; 2.2e-16 These plots provide a check for the main assumptions of the linear regression model. par(mfrow=c(2,2)) # plot all 4 plots in one plot(lm_fit$fit, pch = 16, # optional parameters to make points blue col = &#39;#006EA1&#39;) Data frame of estimated coefficients pa&lt;- tidy(lm_fit) view(pa) Performance metrics on training data glance(lm_fit) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.448 0.445 0.589 166. 0 24 -4389. 8831. 9000. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; vip(lm_fit) Evaluating Test Set Accuracy predict(lm_fit, new_data = data_test) ## # A tibble: 1,652 × 1 ## .pred ## &lt;dbl&gt; ## 1 8.65 ## 2 8.65 ## 3 9.94 ## 4 9.35 ## 5 9.67 ## 6 8.90 ## 7 9.63 ## 8 8.85 ## 9 9.06 ## 10 9.98 ## # … with 1,642 more rows data_test_results &lt;- predict(lm_fit, new_data = data_test) %&gt;% bind_cols(data_test) # View results data_test_results ## # A tibble: 1,652 × 21 ## .pred lntot lneduc finance mobile_own educ iddir marital religion age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 8.65 8.47 5.21 No Access t… No Non … No Married Non Mus… 35 ## 2 8.65 9.36 0 Access to F… No Non … No Married Non Mus… 53 ## 3 9.94 9.08 3.54 Access to F… Yes Prim… No Non ma… Non Mus… 27 ## 4 9.35 9.45 4.54 No Access t… No Prim… No Non ma… Non Mus… 36 ## 5 9.67 9.90 0 No Access t… No Non … No Non ma… Non Mus… 60 ## 6 8.90 8.97 0 No Access t… No Non … No Married Non Mus… 32 ## 7 9.63 9.09 0 No Access t… Yes Prim… No Married Non Mus… 36 ## 8 8.85 8.46 4.84 No Access t… Yes Non … No Married Non Mus… 50 ## 9 9.06 8.34 4.98 Access to F… Yes Prim… No Married Non Mus… 44 ## 10 9.98 9.42 0 No Access t… Yes Non … No Non ma… Non Mus… 25 ## # … with 1,642 more rows, and 11 more variables: ag2 &lt;dbl&gt;, gender &lt;fct&gt;, ## # hh_size &lt;dbl&gt;, hh_size2 &lt;dbl&gt;, know_account &lt;fct&gt;, wage_self &lt;fct&gt;, ## # credit_info &lt;fct&gt;, urban &lt;fct&gt;, bank_com &lt;fct&gt;, educ_com &lt;dbl&gt;, lndis &lt;dbl&gt; Calculating RMSE and R2 on the Test Data RMSE on test set rmse(data_test_results, truth = lntot, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.573 R2 on test set rsq(data_test_results, truth = lntot, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rsq standard 0.468 R2 plot ggplot(data = data_test_results, mapping = aes(x = .pred, y = lntot)) + geom_point(color = &#39;#006EA1&#39;) + geom_abline(intercept = 0, slope = 1, color = &#39;orange&#39;) + labs(title = &#39;Linear Regression Results - Consumption Test Set&#39;, x = &#39;Predicted consumption&#39;, y = &#39;Actual consumption&#39;) 5.2 Deep learning 5.3 Text mining 5.3.1 Sentiment Analysis Sentiment Analysis (SA) extracts information on emotion or opinion from natural language (Silge and Robinson 2017). Most forms of SA provides information about positive or negative polarity, e.g. whether a tweet is positive or negative. This tutorial leverages the data provided in the janeaustenr package. This package contains the complete text of Jane Austen’s 6 completed, published novels, formatted to be convenient for text analysis. The tidytext package contains three sentiment lexicons in the sentiments dataset. The three lexicons are AFINN from Finn Årup Nielsen bing from Bing Liu and collaborators nrc from Saif Mohammad and Peter Turney library(tidytext) library(janeaustenr) library(dplyr) library(stringr) library(ggpubr) library(textdata) tidy_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate( linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(word, text) tidy_books %&gt;% group_by(book) %&gt;% count() ## # A tibble: 6 × 2 ## # Groups: book [6] ## book n ## &lt;fct&gt; &lt;int&gt; ## 1 Sense &amp; Sensibility 119957 ## 2 Pride &amp; Prejudice 122204 ## 3 Mansfield Park 160460 ## 4 Emma 160996 ## 5 Northanger Abbey 77780 ## 6 Persuasion 83658 nrc_se &lt;- get_sentiments(&quot;nrc&quot;) nrc_se %&gt;% group_by(sentiment) %&gt;% count() ## # A tibble: 10 × 2 ## # Groups: sentiment [10] ## sentiment n ## &lt;chr&gt; &lt;int&gt; ## 1 anger 1246 ## 2 anticipation 837 ## 3 disgust 1056 ## 4 fear 1474 ## 5 joy 687 ## 6 negative 3318 ## 7 positive 2308 ## 8 sadness 1187 ## 9 surprise 532 ## 10 trust 1230 nrc_bing &lt;- get_sentiments(&quot;bing&quot;) nrc_bing %&gt;% group_by(sentiment) %&gt;% count() ## # A tibble: 2 × 2 ## # Groups: sentiment [2] ## sentiment n ## &lt;chr&gt; &lt;int&gt; ## 1 negative 4781 ## 2 positive 2005 nrc_afinn &lt;- get_sentiments(&quot;afinn&quot;) nrc_afinn %&gt;% group_by(value) %&gt;% count() ## # A tibble: 11 × 2 ## # Groups: value [11] ## value n ## &lt;dbl&gt; &lt;int&gt; ## 1 -5 16 ## 2 -4 43 ## 3 -3 264 ## 4 -2 966 ## 5 -1 309 ## 6 0 1 ## 7 1 208 ## 8 2 448 ## 9 3 172 ## 10 4 45 ## 11 5 5 nrc_joy &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;joy&quot;) tidy_books %&gt;% filter(book == &quot;Emma&quot;) %&gt;% inner_join(nrc_joy) %&gt;% count(word, sort = TRUE) ## Joining, by = &quot;word&quot; ## # A tibble: 301 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 good 359 ## 2 friend 166 ## 3 hope 143 ## 4 happy 125 ## 5 love 117 ## 6 deal 92 ## 7 found 92 ## 8 present 89 ## 9 kind 82 ## 10 happiness 76 ## # … with 291 more rows library(tidyr) jane_austen_sentiment &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(book, index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) ## Joining, by = &quot;word&quot; create an index that breaks up each book by 500 words; this is the approximate number of words on every two pages so this will allow us to assess changes in sentiment even within chapters library(ggplot2) ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) pride_prejudice &lt;- tidy_books %&gt;% filter(book == &quot;Pride &amp; Prejudice&quot;) pride_prejudice ## # A tibble: 122,204 × 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Pride &amp; Prejudice 1 0 pride ## 2 Pride &amp; Prejudice 1 0 and ## 3 Pride &amp; Prejudice 1 0 prejudice ## 4 Pride &amp; Prejudice 3 0 by ## 5 Pride &amp; Prejudice 3 0 jane ## 6 Pride &amp; Prejudice 3 0 austen ## 7 Pride &amp; Prejudice 7 1 chapter ## 8 Pride &amp; Prejudice 7 1 1 ## 9 Pride &amp; Prejudice 10 1 it ## 10 Pride &amp; Prejudice 10 1 is ## # … with 122,194 more rows afinn &lt;- pride_prejudice %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;% group_by(index = linenumber %/% 80) %&gt;% summarise(sentiment = sum(value)) %&gt;% mutate(method = &quot;AFINN&quot;) ## Joining, by = &quot;word&quot; bing_and_nrc &lt;- bind_rows( pride_prejudice %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% mutate(method = &quot;Bing et al.&quot;), pride_prejudice %&gt;% inner_join(get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) ) %&gt;% mutate(method = &quot;NRC&quot;)) %&gt;% count(method, index = linenumber %/% 80, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(sentiment = positive - negative) ## Joining, by = &quot;word&quot; ## Joining, by = &quot;word&quot; bind_rows(afinn, bing_and_nrc) %&gt;% ggplot(aes(index, sentiment, fill = method)) + geom_col(show.legend = FALSE) + facet_wrap(~method, ncol = 1, scales = &quot;free_y&quot;) get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% count(sentiment) ## # A tibble: 2 × 2 ## sentiment n ## &lt;chr&gt; &lt;int&gt; ## 1 negative 3318 ## 2 positive 2308 get_sentiments(&quot;bing&quot;) %&gt;% count(sentiment) ## # A tibble: 2 × 2 ## sentiment n ## &lt;chr&gt; &lt;int&gt; ## 1 negative 4781 ## 2 positive 2005 bing_word_counts &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% ungroup() ## Joining, by = &quot;word&quot; bing_word_counts %&gt;% group_by(sentiment) %&gt;% slice_max(n, n = 10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(x = &quot;Contribution to sentiment&quot;, y = NULL) + theme_pubr() custom_stop_words &lt;- bind_rows(tibble(word = c(&quot;miss&quot;), lexicon = c(&quot;custom&quot;)), stop_words) library(wordcloud) ## Loading required package: RColorBrewer tidy_books %&gt;% anti_join(stop_words) %&gt;% count(word) %&gt;% with(wordcloud(word, n, max.words = 100)) ## Joining, by = &quot;word&quot; library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100) ## Joining, by = &quot;word&quot; "],["trade-policy-analysis-with-r.html", "Chapter 6 Trade policy analysis with R 6.1 Analyzing trade flow 6.2 Analyzing trade tariff 6.3 The gravity model of Trade", " Chapter 6 Trade policy analysis with R 6.1 Analyzing trade flow 6.2 Analyzing trade tariff 6.3 The gravity model of Trade "],["quantitative-finance-with-r.html", "Chapter 7 Quantitative Finance with R 7.1 Modern portfolio theory 7.2 Financial models 7.3 Analyze financial assets to find their Return On Investment (ROI) 7.4 profit-making strategy", " Chapter 7 Quantitative Finance with R 7.1 Modern portfolio theory 7.2 Financial models 7.3 Analyze financial assets to find their Return On Investment (ROI) 7.4 profit-making strategy "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
